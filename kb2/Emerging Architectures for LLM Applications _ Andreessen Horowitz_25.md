**Source:** `Emerging Architectures for LLM Applications _ Andreessen Horowitz`

**Inference Options for Open Source Models**
Inference options for open-source models include simple API interfaces from Hugging Face and Replicate, raw compute resources from cloud providers, and opinionated cloud offerings. â€” [Matt Bornstein and Rajko Radovanovic, Andreessen Horowitz]
