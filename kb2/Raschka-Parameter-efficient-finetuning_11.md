**Source:** `Raschka-Parameter-efficient-finetuning`

**LLaMA-Adapter: Combining Prefix Tuning and Adapters**
LLaMA-Adapter prepends tunable prompt tensors to the embedded inputs, similar to prefix tuning, but also introduces a zero-initialized attention mechanism and gating. â€” Raschka-Parameter-efficient-finetuning
