**Source:** `Raschka-Parameter-efficient-finetuning`

**LLaMA-Adapter Training Stability**
LLaMA-Adapter uses a gating mechanism to stabilize training by mitigating the potential disruption of linguistic knowledge caused by randomly initialized tensors. â€” Raschka-Parameter-efficient-finetuning
