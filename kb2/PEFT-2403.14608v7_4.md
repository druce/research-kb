**Source:** `PEFT-2403.14608v7`

**Supporting Fact: Standard Full Fine-Tuning is Highly Inefficient for Large Models**
Full fine-tuning of LLMs requires thousands of GPUs working in parallel, which is highly inefficient and unsustainable. PEFT aims to tune minimal parameters to achieve better performance.
