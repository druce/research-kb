**Source:** `2023 survey & evolutionary tree`

**Trustworthiness: Safety Concerns**
Safety concerns associated with LLMs should be given utmost importance as the potentially harmful or biased outputs, and hallucinations from LLMs can result in severe consequences.
