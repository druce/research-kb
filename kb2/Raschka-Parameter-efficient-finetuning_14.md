**Source:** `Raschka-Parameter-efficient-finetuning`

**LLaMA-Adapter Performance and Efficiency**
A 7 billion parameter LLaMA model can be finetuned in one hour using eight A100 GPUs with LLaMA-Adapter, outperforming other models on question-answering tasks while only finetuning 1.2M parameters. â€” Raschka-Parameter-efficient-finetuning
