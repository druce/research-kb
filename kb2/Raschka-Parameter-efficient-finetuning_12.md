**Source:** `Raschka-Parameter-efficient-finetuning`

**LLaMA-Adapter: Focus on Top Layers**
LLaMA-Adapter adds learnable adaption prompts only to the topmost transformer layers, enabling more effective tuning of language representations. â€” Raschka-Parameter-efficient-finetuning
