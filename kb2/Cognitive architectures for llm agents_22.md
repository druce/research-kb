**Source:** `Cognitive architectures for llm agents`

**Measuring Success: Safety and Alignment**
Address issues such as over-confidence, miscalibration, misalignment with human values, hallucinations in self-evaluation, and lack of human-in-the-loop mechanisms to improve LLMs' utilities as agent backbones.
