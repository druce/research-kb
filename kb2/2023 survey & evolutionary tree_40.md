**Source:** `2023 survey & evolutionary tree`

**Safety: Hallucinations**
The potential for LLMs to "hallucinate," or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications.
