**Source:** `A jargon-free explanation of how AI large language models work - Ars Technica`

**Attention Heads Focus on Different Tasks**
Each attention layer has multiple "attention heads" that focus on different tasks, such as matching pronouns with nouns, resolving homonyms, or linking two-word phrases. â€” [Timothy B. Lee and Sean Trott, Ars Technica]
