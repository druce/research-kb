**Source:** `A jargon-free explanation of how AI large language models work - Ars Technica`

**Attention Heads vs. Feed-Forward Layers**
Attention heads retrieve information from earlier words in a prompt, while feed-forward layers enable language models to "remember" information that's not in the prompt. â€” [Timothy B. Lee and Sean Trott, Ars Technica]
