**Source:** `securing-ai`

**Large Language Model Misuse: Jailbreaking**
Adversaries override an LLM’s instructions and safety alignment through jailbreaking. — [What do AI Security Threats Look Like?]
