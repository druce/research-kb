# Research Notes: situationalawareness

**Source:** `situationalawareness`  
**Processed:** research-kb

---

Okay, here are the extracted insights from the document, formatted as individual notes:

**Key Finding: AGI by 2027 Plausible**
AGI, capable of AI researcher/engineer work, is strikingly plausible by 2027, based on extrapolating trends in compute, algorithmic efficiencies, and "unhobbling" gains. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Key Finding: Superintelligence Could Follow AGI Quickly**
Automated AI research, with millions of AGIs, could compress a decade of algorithmic progress into a year, leading to rapid transition from AGI to vastly superhuman AI systems. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Key Finding: Trillions Will Be Invested in AI Infrastructure**
As AI revenue grows rapidly, trillions of dollars will be invested in GPU, datacenter, and power buildout before the end of the decade. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Key Finding: AI Labs' Security is an Afterthought**
Leading AI labs treat security as an afterthought, handing AGI secrets to the CCP on a silver platter. Securing AGI secrets and weights against state actors is crucial. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Key Finding: Superalignment is an Unsolved Problem**
Reliably controlling AI systems much smarter than we are is an unsolved technical problem. Failure to solve it could easily be catastrophic. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Key Finding: Free World Must Prevail in AGI Race**
Superintelligence will give a decisive economic and military advantage. The free world's very survival will be at stake in the race to AGI. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Key Quote: Models Just Want to Learn**
"The models, they just want to learn. You have to understand this. The models, they just want to learn." — Ilya Sutskever (circa 2015, via Dario Amodei), highlighting the consistent improvements with scaling deep learning.

---

**Supporting Fact: GPT-2 to GPT-4 Qualitative Jump**
GPT-2 to GPT-4 represented a qualitative jump from ~preschooler to ~smart high-schooler abilities in 4 years. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Supporting Fact: Compute Scaling Trend**
Training compute for frontier AI systems has grown at roughly ~0.5 OOMs/year for the last decade and a half. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Supporting Fact: Algorithmic Efficiency Gains**
Algorithmic efficiency in language modeling has improved by roughly ~0.5 OOMs/year from 2012 to 2023. — Epoch AI estimates, suggesting significant compute multipliers.

---

**Supporting Fact: Unhobbling Gains**
Techniques like scaffolding and tool use can result in effective compute gains of 5-30x on many benchmarks. — Epoch AI survey, highlighting the importance of algorithmic progress beyond base models.

---

**AI Roadmap: Three Key Ingredients for Agent-Coworkers**
The path from chatbot to agent-coworker requires: 1) solving the "onboarding problem," 2) addressing the test-time compute overhang, and 3) enabling models to use a computer. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Critical Capability: Securing Algorithmic Secrets**
AGI-level security for algorithmic secrets is necessary years before AGI-level security for weights. These algorithmic breakthroughs will matter more than a 10x or 100x larger cluster in a few years. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Critical Capability: Superalignment Research**
We will need better alignment techniques to ensure even basic side constraints for future models, like follow instructions or follow the law. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Critical Capability: Industrial Mobilization**
The race to AGI will require mobilizing American industrial might, including building massive GPU clusters, datacenters, and power plants. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**AI Roadmap: Stages of AGI Development**
The author envisions a progression: 1) Proto-automated engineers (2026/27), 2) Proto-automated researchers (2027/28), and 3) 10x+ pace of progress leading to superintelligence (2028/29). — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Decisions: Prioritize Data Centers in the US**
Given the dysfunction and cost of building fabs in the US, prioritize datacenters in the US while betting more heavily on democratic allies like Japan and South Korea for fab projects. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Decisions: Prioritize Security**
American AI labs must put the national interest first, before the allure of free-flowing Middle Eastern cash, arcane regulation, or even, yes, admirable climate commitments. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Best Practice: Superdefense**
Even with alignment, implement "superdefense" measures like airgapped datacenters, hardware encryption, and strict limitations on external dependencies to reduce fallout from potential failures. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**Measuring Success: AGI Revenue Run Rate**
A key milestone for AI revenue is when a big tech company (Google, Microsoft, Meta, etc.) hits a $100B revenue run rate from AI (products and API). — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---

**AI Roadmap: The Project**
By 2027/28, expect a government AGI project to emerge, as no startup can handle superintelligence. The endgame will be on somewhere in a SCIF. — Leopold Aschenbrenner, Situational Awareness: The Decade Ahead

---