# Research Notes: securing-ai

**Source:** `securing-ai`  
**Processed:** research-kb

---

**AI Security: A New Cyber Threat Landscape**
The accelerating adoption of AI systems introduces new threats and vulnerabilities, motivating leaders to revisit the security status quo. — [Introduction]
---
**AI Systems Support Strategic Decision Making**
AI systems are increasingly responsible for supporting strategic decision-making, making them targets for adversaries seeking to degrade, deny, deceive, or manipulate them. — [Introduction]
---
**Increased AI Adoption = Increased Risk**
The more AI-driven use cases an organization implements, the more AI integrations fall into the crosshairs of adversaries, bringing increased risk of mission failure and reputational damage. — [Introduction]
---
**AI and Cybersecurity Talent Shortage**
Organizations face a shortage of AI and cybersecurity talent, particularly those who understand the security risks of LLMs and can collaborate effectively with data scientists and AI engineers. — [Introduction]
---
**Key Quote: AI Security Expertise is Rare**
"Those who understand the security risks of LLMs and can collaborate effectively with data scientists and AI engineers '. . . is a much smaller and rarer group of people.'" — [Ben Aung, chief risk officer at Sage, via Wall Street Journal]
---
**AI Skills Shortage: Prompt Injection**
O’Reilly’s State of Security in 2024 study found that 33.9% of respondents identified a shortage of AI skills, particularly for vulnerabilities like prompt injection, as one of their most significant gaps. — [Introduction]
---
**Centralized AI Security Expertise**
By creating a centralized cadre of AI security experts and empowering them to collaborate with business and IT stakeholders, organizations can maximize the reach and effectiveness of this limited pool of expertise. — [Introduction]
---
**Cross-Functional Governance for AI Security**
With a cross-functional governance approach, an awareness of the latest threats, and the staffing and ability to implement calibrated countermeasures, organizations can confidently turn an AI-first posture into a secure mission transformation. — [Introduction]
---
**Ubiquitous AI Magnifies Vulnerabilities**
The distributed and ubiquitous nature of AI magnifies its vulnerabilities, especially with AI agents embedded into business processes with limited oversight. — [Why is AI Security Unique?]
---
**Shadow AI and Third-Party Risks**
Employees may be doing shadow AI (using AI tools without approval), and vendors may be embedding AI without notifying users, decreasing transparency and weakening security. — [Why is AI Security Unique?]
---
**Anomaly Detection Limitations in AI Security**
Organizations can't rely as much on anomaly detection to discover malicious intent in AI systems because AI systems often act stochastically and produce unpredictable results. — [Why is AI Security Unique?]
---
**GenAI's Non-Deterministic Nature Adds Complexity**
The non-deterministic nature of GenAI adds complexity to security testing by multiplying error scenarios, making it difficult to assess every imaginable attack path. — [Why is AI Security Unique?]
---
**Defense-in-Depth, Secure-by-Design, and Zero Trust as Foundation**
Enterprises can leverage strategies like defense-in-depth, secure-by-design, and zero trust as a solid foundation for securing AI systems. — [Why is AI Security Unique?]
---
**AI Models: Complex Software Modules**
AI models are the most complex software modules ever created, making it more difficult to find, analyze, and remediate attacks. — [Why is AI Security Unique?]
---
**AI "Black Box" Hides Cybersecurity Information**
The "black box" nature of AI models makes it difficult to understand how inputs become outputs, hindering security professionals who need visibility into application function and data usage. — [Why is AI Security Unique?]
---
**Third-Party AI Model Risks**
Enterprises often procure AI models from third-party providers, assuming built-in security, but providers may have different security priorities than the enterprise. — [Why is AI Security Unique?]
---
**Open-Source AI Model Risks**
Organizations downloading pretrained, open-source AI models may overlook malware and other threats, requiring them to find their own security solutions. — [Why is AI Security Unique?]
---
**AI Systems Must Adhere to Established Security Requirements**
Despite their unique characteristics, AI systems remain enterprise systems and must adhere to established security requirements. — [Who is Responsible for AI Security?]
---
**AI Security Engineering Augments Traditional Security**
Traditional risk, DevSecOps, and cybersecurity teams will be supported and augmented by AI security engineering teams with expertise in core AI operations and special risks. — [Who is Responsible for AI Security?]
---
**Gartner: IT/Security Leaders Involved in GenAI Security**
Nearly all (93%) of IT/security leaders surveyed are at least somewhat involved in their organization’s GenAI security/risk management efforts, but just 24% said they own this responsibility. — [Gartner Report, Who is Responsible for AI Security?]
---
**IBM IBV: Secure AI is Essential, But Not Implemented**
An IBM IBV survey found that while 82% of respondents say secure and trustworthy AI is essential, just 24% of their current generative AI projects have a component to secure these initiatives. — [IBM IBV Survey, Who is Responsible for AI Security?]
---
**Integrated Approach to Secure Systems**
Leading enterprises employ an integrated approach that assesses potential risks, implements critical guardrails, hardens systems during design and development, and actively monitors and defends against potential threats. — [How AI Security Engineers Reinforce the Security Foundation]
---
**AI Security Engineers Enhance GRC**
AI security engineers can enhance GRC for AI-related risks by monitoring for data privacy, assisting with compliance, and integrating AI risk management with broader GRC guardrails. — [How AI Security Engineers Reinforce the Security Foundation]
---
**AI Security Engineers Strengthen Model Training and Architectures**
AI security engineers can ensure more robust model training and architectures to strengthen their attack resilience. — [How AI Security Engineers Reinforce the Security Foundation]
---
**AI Security Engineers Perform Deep Testing and Monitoring**
AI security engineers can perform deep testing and monitoring of AI systems to detect suspicious behavior that might be otherwise overlooked due to the technology’s non-deterministic nature. — [How AI Security Engineers Reinforce the Security Foundation]
---
**AI Security Lapses Lead to Reputational Damage and Financial Liability**
AI security lapses and glitches take various forms, and even the world’s most well-funded AI innovators are not immune, resulting in reputational damage and financial liability. — [What do AI Security Threats Look Like?]
---
**Examples of AI Security Lapses**
Examples include: TrojanPuzzle attacks, ChatGPT data leaks, malicious dependency packages, and fooled facial authentication models. — [What do AI Security Threats Look Like?]
---
**Need for Controls for Safe AI Integration**
These malicious actions and algorithmic errors highlight the critical need for organizations to implement controls that allow safe integration of AI with core systems and processes. — [What do AI Security Threats Look Like?]
---
**Adversarial Personas Targeting AI**
Adversarial personas range from individual threat actors and hacktivists to financially motivated criminal organizations and nation-states. — [What do AI Security Threats Look Like?]
---
**Key Finding: Catastrophic AI Attack is a Matter of "When," Not "If"**
Many experts believe a catastrophic attack on AI systems is a matter of "when" not "if," given the value of the information underlying these systems and their overall risk profile. — [What do AI Security Threats Look Like?]
---
**Limited AI Deployments Due to Security Concerns**
A comparatively smaller number of AI systems are deployed in full production, given challenges in certifying them as fully secure to achieve authority to operate (ATO). — [What do AI Security Threats Look Like?]
---
**AI Attack Surface is Inviting**
The attack surface continues to be inviting, as these types of attacks may in some cases be easier to carry out than breaking into web servers or overwhelming networks. — [What do AI Security Threats Look Like?]
---
**Data Poisoning: Manipulating Training Data**
Adversaries manipulate training data to compromise model behaviors and insert backdoors through data poisoning. — [What do AI Security Threats Look Like?]
---
**Malware in AI Models**
Adversaries package malicious code within model files and libraries, similar to malware in any other file. — [What do AI Security Threats Look Like?]
---
**Model Evasion: Fooling the Model**
Adversaries perturb model inputs to control model outputs, also known as model evasion. — [What do AI Security Threats Look Like?]
---
**Large Language Model Misuse: Jailbreaking**
Adversaries override an LLM’s instructions and safety alignment through jailbreaking. — [What do AI Security Threats Look Like?]
---
**Data Leakage and Model Theft**
Adversaries infer and steal sensitive training data, model behavior, and/or intellectual property through data leakage and model theft. — [What do AI Security Threats Look Like?]
---
**One-Size-Fits-All AI Security Strategy is Ineffective**
Given the diversity of threats and threat actors, enterprises cannot simply embrace a one-size-fits-all strategy. — [Getting Started with an AI Security Strategy]
---
**MITRE ATLAS for Benchmarking AI Security**
Booz Allen often uses a comprehensive security framework such as MITRE ATLAS to benchmark the current state and identify realistic objectives that meaningfully enhance the security posture. — [Getting Started with an AI Security Strategy]
---
**Key AI Security Engineering Practices**
Key practices include: Risk Modeling, Red Teaming, Security Testing, Model Scanning, Dependency Scanning, Data Tampering Detection, Robust Model Training, Operational Controls and Monitoring, Model Updates, and Governance. — [Getting Started with an AI Security Strategy]
---
**Tailored Analysis for AI Security Practices**
Organizations should conduct a tailored analysis to identify what AI security practices to incorporate rather than setting out to apply each practice. — [Getting Started with an AI Security Strategy]
---
**Qualify and Control Risks Through Risk Modeling and Governance**
At a minimum, organizations must qualify and control risks by performing risk modeling and establishing a governance plan. — [Getting Started with an AI Security Strategy]
---
**Open-Source Tools for Model and Dependency Scanning**
Open-source tools for model and dependency scanning can be incorporated at very low cost and quickly integrated into the MLOps pipeline. — [Getting Started with an AI Security Strategy]
---
**Risk Profiles for Different AI Use Cases**
Different risk profiles exist for: GenAI for Business, Third-Party GenAI Model Integration, Pretrained Models, Fine-Tuned Models, and Homegrown Models. — [Getting Started with an AI Security Strategy]
---
**AI Security: A Team Effort**
AI security must be a team effort, requiring collaboration across CIOs, chief risk officers, CTOs, and CISOs, as well as AI security engineering expertise. — [Everyone Has a Role to Play in AI Security]
---
**Updating Operating Policies for AI**
The inclusion of AI technologies within enterprise systems requires that organizations update some of their most critical operating policies. — [Everyone Has a Role to Play in AI Security]
---
**Critical Operating Policies for AI**
Critical operating policies include: Integrated Oversight and Management, Risk Assessment, Governance and Policy, Continuous Improvement, and Upskilling. — [Everyone Has a Role to Play in AI Security]
---
**Continuous Improvement and Upskilling are Vital for AI Security**
The dynamic nature of AI technology necessitates continuous learning and improvement in security practices. — [Everyone Has a Role to Play in AI Security]
---
**Regulators and Standards Organizations Offer AI Security Resources**
Regulators and standards organizations like NIST, MITRE, and OWASP offer frameworks, tools, and knowledge bases for AI security. — [Appendix A: Mobilizing to Advance AI Security Resources]
---
**Expansion of AI Security Threats Catalyzes Growth in Countermeasures**
The expansion of AI security threats has catalyzed a corresponding growth in tools for countering them. — [Appendix A: Mobilizing to Advance AI Security Resources]
---
**Biden Administration's Executive Order on AI**
The Biden Administration’s Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence calls for robust, reliable, repeatable, and standardized evaluations of AI systems before they are operationalized. — [Appendix A: Mobilizing to Advance AI Security Resources]